{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm8s50W96ZVaztf/ab5ueL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/email_spam_classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "x0vwzoK4vjeI"
      },
      "outputs": [],
      "source": [
        "# Install opendatasets library\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from tensorflow.keras.layers import Dense,Embedding,Bidirectional,GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.metrics import Precision,Recall\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "Z9b6joOR8nlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into the working directory\n",
        "od.download(\"https://www.kaggle.com/datasets/purusinghvi/email-spam-classification-dataset\")"
      ],
      "metadata": {
        "id": "H_Cprs_qvmPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data in a pandas frame\n",
        "path_to_data=\"/content/email-spam-classification-dataset/combined_data.csv\"\n",
        "data=pd.read_csv(path_to_data)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "AwW2Tm9nvmRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the data\n",
        "print(\"This data contains: \"+str(data.shape[0])+\" rows and \"+str(data.shape[1])+\" columns.\")"
      ],
      "metadata": {
        "id": "6M7ukqrvvmUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the frequency of each class\n",
        "data['label'].value_counts()"
      ],
      "metadata": {
        "id": "gVvWmop0oNOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for nulls\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "2YivpdijvmW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRN4WqkltlB5"
      },
      "source": [
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define PorterStemmer and the stopwords\n",
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "BQcBPtAI89BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKKIsHqZwRJR"
      },
      "source": [
        "# define cleaning function\n",
        "def clean_text(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove non alphabetic characters ^\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # tokenize sentences\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Porter Stemmer\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  # reconstruct the text\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "  return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define count_words function to find the count of tokens in a sentence\n",
        "def count_words(text):\n",
        "  return len(text.split())"
      ],
      "metadata": {
        "id": "khmORk_-FEs6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply count_words function on text-column\n",
        "data['text_length']=data['text'].apply(count_words)"
      ],
      "metadata": {
        "id": "mrrwP7JOEBHo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the maximum length of a text in the data\n",
        "print(\"The maximum lenght of a text is: \"+str(data['text_length'].max()))"
      ],
      "metadata": {
        "id": "8QzMLNcdyY2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply clean_text on text-column\n",
        "data['clean_text']=data['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "jIWq8vgPyY6Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer and fit the cleaned_text\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(data['clean_text'])\n",
        "#Convert texts to sequences\n",
        "x=tokenizer.texts_to_sequences(data['clean_text'])\n",
        "# Pad the sequences\n",
        "x = pad_sequences(x, maxlen=1000, padding=\"post\", truncating=\"post\")\n",
        "# Set the labels\n",
        "y=data['label'].values\n",
        "# Check the input vocab size\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "3O_NJ2bj0_er"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,100,input_length=1000))\n",
        "model.add(Bidirectional(GRU(65,return_sequences=False)))\n",
        "model.add(Dense(65,activation='relu'))\n",
        "model.add(Dense(25,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "# Check the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_gIhz1UUvmZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into testing and training parts\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "FOwFtXKV0lGv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model and fit it the training data\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy',Precision(),Recall()])\n",
        "model.fit(x_train,y_train,validation_split=0.2,epochs=3,batch_size=64)"
      ],
      "metadata": {
        "id": "Gwl5bW0Z6aAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on testing data\n",
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "id": "nrq4w4FL6aEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a spam_predictor function\n",
        "def spam_predictor(input_text, model, tokenizer):\n",
        "    # Clean,tokenize and pad the input text\n",
        "    input_text_clean=clean_text(input_text)\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text_clean])\n",
        "    input_padded = pad_sequences(input_sequence, maxlen=1000, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # Make predictions using the pre-trained model\n",
        "    predictions = model.predict(input_padded)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Q9ANFT50vmca"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use spam_predictor function\n",
        "input_text='You have been selected to win the  grand prize winner of our exclusive lottery! Claim your prize by contacting us with your personal information'\n",
        "spam_predictor(input_text, model, tokenizer)"
      ],
      "metadata": {
        "id": "9ZMlOvQHvmf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "# Pickle the predictor function\n",
        "with open(\"spam_predictor.pkl\", \"wb\") as file:\n",
        "    pickle.dump(spam_predictor, file)\n",
        "\n",
        "# Pickle the tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as file:\n",
        "    pickle.dump(tokenizer, file)\n"
      ],
      "metadata": {
        "id": "gjGlZdDWE8SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data again\n",
        "x = data['clean_text'].values\n",
        "y = data['label'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "soe_a_WekMHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CountVectorizer (with binary=True and max_features=10000)\n",
        "vectorizer = CountVectorizer(binary = True, max_features = 10000)\n",
        "# Learn the vocabulary of all tokens in our training dataset\n",
        "vectorizer.fit_transform(x_train)\n",
        "\n",
        "# Transform x_train to bag of words\n",
        "x_train_bow = vectorizer.transform(x_train)\n",
        "x_test_bow = vectorizer.transform(x_test)\n",
        "\n",
        "print(x_train_bow.shape, y_train.shape)\n",
        "print(x_test_bow.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "goB9vJhSkMKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defin LogisticRegression model\n",
        "LR=LogisticRegression()\n",
        "LR.fit(x_train_bow,y_train)"
      ],
      "metadata": {
        "id": "RyTZcjwHkMPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using LR on training and testing data\n",
        "y_hat_test = LR.predict(x_test_bow)\n",
        "y_hat_train = LR.predict(x_train_bow)\n",
        "\n",
        "# Calculate, accuracy,precisison and recall scores on training and testing datasets\n",
        "accuracy_train = accuracy_score(y_train, y_hat_train) * 100\n",
        "precision_train = precision_score(y_train, y_hat_train, average='binary', zero_division=0) * 100\n",
        "recall_train = recall_score(y_train, y_hat_train, average='binary', zero_division=0) * 100\n",
        "\n",
        "accuracy_test = accuracy_score(y_test, y_hat_test) * 100\n",
        "precision_test = precision_score(y_test, y_hat_test, average='binary', zero_division=0) * 100\n",
        "recall_test = recall_score(y_test, y_hat_test, average='binary', zero_division=0) * 100\n",
        "\n",
        "print(\"Scores on training data:\")\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_train))\n",
        "print(\"Precision: {:.2f}%\".format(precision_train))\n",
        "print(\"Recall: {:.2f}%\".format(recall_train))\n",
        "print(\"----------------------------\")\n",
        "print(\"Scores on testing data:\")\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_test))\n",
        "print(\"Precision: {:.2f}%\".format(precision_test))\n",
        "print(\"Recall: {:.2f}%\".format(recall_test))\n"
      ],
      "metadata": {
        "id": "llm8jeUfpCxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}