{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFywJqv+meZETA//5ozzoh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/subject_classification_model_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zAwUoBRwTZe"
      },
      "outputs": [],
      "source": [
        "# Install opendatasets library\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import opendatasets as od\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU,Dense,Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "xcxnm8IkwW7T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "kIkRGByA0POq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define useful classes\n",
        "English_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "YwDsrdpC1EkK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into working directory\n",
        "od.download(\"https://www.kaggle.com/datasets/mrutyunjaybiswal/iitjee-neet-aims-students-questions-data\")"
      ],
      "metadata": {
        "id": "IESHlcIOwW95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data in a pandas frame\n",
        "data=pd.read_csv(\"/content/iitjee-neet-aims-students-questions-data/subjects-questions.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "j15DuhNjwXAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distrubution of classes\n",
        "data['Subject'].value_counts()"
      ],
      "metadata": {
        "id": "_Kklb3sKL2dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyrg00Ycb08M"
      },
      "source": [
        "# Define a function to clean texts\n",
        "def clean_text(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove none alphabetic characters\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # stem words\n",
        "  # split into words\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # stemming of words\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in English_stopwords])\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# apply to all dataset\n",
        "data['eng'] = data['eng'].apply(clean_text)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing parts\n",
        "x=data['eng'].values\n",
        "y=data['Subject'].values\n",
        "encoder=LabelEncoder()\n",
        "y=encoder.fit_transform(y)# Encode the labels\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=42)\n"
      ],
      "metadata": {
        "id": "PLFgBhVtwXKu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "metadata": {
        "id": "xwZ0fkDrwXSw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer\n",
        "tok=Tokenizer(num_words=10000)\n",
        "tok.fit_on_texts(x_train)\n",
        "vocab_size=len(tok.word_index)+1"
      ],
      "metadata": {
        "id": "XR3X53PT5Qs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences\n",
        "x_train=tok.texts_to_sequences(x_train)\n",
        "x_test=tok.texts_to_sequences(x_test)\n"
      ],
      "metadata": {
        "id": "uR4qQ8og52nD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the defined Squences\n",
        "x_train=pad_sequences(x_train,maxlen=80,padding=\"post\")\n",
        "x_test=pad_sequences(x_test,maxlen=80,padding=\"post\")\n"
      ],
      "metadata": {
        "id": "tinocCIF6Ad2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate schedule function\n",
        "def learning_rate_schedule(epoch):\n",
        "    initial_lr = 0.01  # Initial learning rate\n",
        "    decay = 0.1\n",
        "    if epoch < 5:\n",
        "        return initial_lr\n",
        "    else:\n",
        "        return initial_lr * np.power(decay, (epoch //5))\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Create a LearningRateScheduler callback\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
        "\n",
        "# Define and compile your model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,input_length=80,output_dim=70))\n",
        "model.add(GRU(75,return_sequences=False))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train your model with both callbacks\n",
        "history=model.fit(x_train, y_train, epochs=10,validation_split=0.2, callbacks=[early_stopping, lr_scheduler],batch_size=32,verbose=1)\n"
      ],
      "metadata": {
        "id": "TMXP1OuF4Fl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on testing data\n",
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "id": "pbkswAq9hD_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the model architecture to a file (e.g., model.png)\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "DNbyGIlEFc-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract training and validation loss and accuracy values\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss, label='Training Loss',c=\"red\")\n",
        "plt.plot(val_loss, label='Validation Loss',c=\"blue\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Plot the accuracy curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracy, label='Training Accuracy',c=\"red\")\n",
        "plt.plot(val_accuracy, label='Validation Accuracy',c=\"blue\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6VGl_3Fsd8Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subject_predictor(input_text, model, tok, max_length=80):\n",
        "    # Clean the input text\n",
        "    cleaned_input = clean_text(input_text)\n",
        "\n",
        "    # Tokenize the cleaned text using a pre-trained tokenizer (tok)\n",
        "    input_sequence = tok.texts_to_sequences([cleaned_input])\n",
        "\n",
        "    # Pad the sequence to a fixed length (max_length)\n",
        "    padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "    # Make predictions using the pre-trained model (model)\n",
        "    predictions = model.predict(padded_input_sequence)\n",
        "\n",
        "    # Assuming you want the predicted class\n",
        "    predicted_class = predictions[0]\n",
        "\n",
        "    return predicted_class\n"
      ],
      "metadata": {
        "id": "UV4MDlOed8KD"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us have some fun!\n",
        "input_text='Can you give me some refrences in quantum mechanics?!'\n",
        "print(subject_predictor(input_text,model,tok,80))"
      ],
      "metadata": {
        "id": "17ORbkjfusV_"
      },
      "execution_count": 102,
      "outputs": []
    }
  ]
}