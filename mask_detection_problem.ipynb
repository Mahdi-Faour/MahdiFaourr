{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgbvLZgiVVMTKifZoLyBQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/mask_detection_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the 'mlflow' library for managing the machine learning lifecycle\n",
        "!pip install mlflow\n",
        "# Install the 'opendatasets' library to load online datasets\n",
        "!pip install opendatasets\n",
        "# Import necessary libraries\n",
        "import mlflow\n",
        "from mlflow import keras\n",
        "import mlflow.keras\n",
        "import opendatasets as od\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pickle\n",
        "#Load the dataset\n",
        "od.download(\"https://www.kaggle.com/datasets/prithwirajmitra/covid-face-mask-detection-dataset\")\n",
        "# Create a new MLflow experiment named \"first_experiment\"\n",
        "experiment = mlflow.create_experiment(\"first_experiment\")\n",
        "# Set the active MLflow experiment to \"first_experiment\"\n",
        "mlflow.set_experiment('first_experiment')\n",
        "\n",
        "# Initialize the ImageDataGenerator with data augmentation and preprocessing settings\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,  # Rescale pixel values to the range [0, 1]\n",
        "    rotation_range=20,  # Randomly rotate images in the range (-20, 20) degrees\n",
        "    width_shift_range=0.1,  # Randomly shift the width by up to 10% of the image width\n",
        "    height_shift_range=0.1,  # Randomly shift the height by up to 10% of the image height\n",
        "    shear_range=0.2,  # Shear transformation with a maximum shear angle of 20 degrees\n",
        "    zoom_range=0.2,  # Randomly zoom in or out by up to 20%\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest',  # Fill empty pixels with the nearest available pixel value\n",
        ")\n",
        "\n",
        "# Create data iterators for training, validation, and testing sets\n",
        "train_iterator = datagen.flow_from_directory(\n",
        "    \"covid-face-mask-detection-dataset/New Masks Dataset/Train\",\n",
        "    class_mode='binary',\n",
        "    batch_size=30,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "valid_iterator = datagen.flow_from_directory(\n",
        "    \"covid-face-mask-detection-dataset/New Masks Dataset/Validation\",\n",
        "    class_mode='binary',\n",
        "    batch_size=30,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_iterator = datagen.flow_from_directory(\n",
        "    \"covid-face-mask-detection-dataset/New Masks Dataset/Test\",\n",
        "    class_mode='binary',\n",
        "    batch_size=30,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define a function to create a CNN model\n",
        "def get_model(params):\n",
        "    # Initialize a Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the first convolutional layer with specified parameters\n",
        "    model.add(Conv2D(params['nb_filters'], params['filter_size'], padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
        "\n",
        "    # Add a second convolutional layer with the same parameters\n",
        "    model.add(Conv2D(params['nb_filters'], params['filter_size'], padding='same', activation='relu'))\n",
        "\n",
        "    # Add a max-pooling layer\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    # Flatten the output for the fully connected layers\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add a dense layer with specified number of neurons and ReLU activation\n",
        "    model.add(Dense(params['nb_neurons'], activation='relu'))\n",
        "\n",
        "    # Add the output layer with a single neuron and sigmoid activation for binary classification\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "# Define a function to train a machine learning model\n",
        "def train_model(params, text_params, train_iterator, valid_iterator):\n",
        "    # Create a model based on the specified parameters\n",
        "    model = get_model(params)\n",
        "\n",
        "    # Compile the model with the specified optimizer, loss function, and metrics\n",
        "    model.compile(optimizer=text_params['optimizer'], loss='binary_crossentropy', metrics=text_params['metrics'])\n",
        "\n",
        "    # Train the model using the training data and validate it using the validation data\n",
        "    history = model.fit(train_iterator, validation_data=valid_iterator, epochs=params['epochs'])\n",
        "\n",
        "    return history, model\n",
        "\n",
        "# Define a function to log experiment information and model using MLflow\n",
        "def mlflow_log(params, history, text_params, model, run_num):\n",
        "    # Extract the metrics from the training history\n",
        "    metrics = history.history\n",
        "\n",
        "    # Get the last value for each metric\n",
        "    for key in metrics.keys():\n",
        "        metrics[key] = metrics[key][-1]\n",
        "\n",
        "    # Start a new MLflow run for the experiment\n",
        "    with mlflow.start_run() as run:\n",
        "        # Log experiment parameters\n",
        "        mlflow.log_params(params)\n",
        "\n",
        "        # Log the metrics from the training history\n",
        "        mlflow.log_metrics(metrics)\n",
        "\n",
        "        # Log textual parameters\n",
        "        for text_item in text_params.items():\n",
        "            if isinstance(text_item[1], list):\n",
        "                for metric_name in text_item[1]:\n",
        "                    mlflow.log_text(metric_name, text_item[0] + '.txt')\n",
        "            else:\n",
        "                mlflow.log_text(text_item[1], text_item[0] + '.txt')\n",
        "\n",
        "        # Log the Keras model as an artifact\n",
        "        mlflow.keras.log_model(model, artifact_path=f\"model_{run_num}\")\n",
        "# Set multiple runs,1st run:\n",
        "params = {\n",
        "    'filter_size':4,\n",
        "    'nb_filters':4,\n",
        "    'nb_neurons':70,\n",
        "    'epochs':4,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'sgd',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num = 1\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "# 2nd run\n",
        "params = {\n",
        "    'filter_size':3,\n",
        "    'nb_filters':3,\n",
        "    'nb_neurons':100,\n",
        "    'epochs':3,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'sgd',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num = 2\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "# 3rd run\n",
        "params = {\n",
        "    'filter_size':3,\n",
        "    'nb_filters':3,\n",
        "    'nb_neurons':70,\n",
        "    'epochs':3,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'adam',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num =3\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "# 4th run\n",
        "params = {\n",
        "    'filter_size':5,\n",
        "    'nb_filters':5,\n",
        "    'nb_neurons':100,\n",
        "    'epochs':6,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'sgd',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num =4\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "# 5th run\n",
        "params = {\n",
        "    'filter_size':3,\n",
        "    'nb_filters':3,\n",
        "    'nb_neurons':110,\n",
        "    'epochs':8,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'sgd',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num = 5\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "#6th run\n",
        "params = {\n",
        "    'filter_size':4,\n",
        "    'nb_filters':3,\n",
        "    'nb_neurons':110,\n",
        "    'epochs':10,\n",
        "    'batch_size':20\n",
        "}\n",
        "\n",
        "text_params = {\n",
        "    'optimizer':'sgd',\n",
        "    'metrics':['accuracy']\n",
        "}\n",
        "\n",
        "run_num = 6\n",
        "\n",
        "history,model=train_model(params,text_params,train_iterator,valid_iterator)\n",
        "\n",
        "print(\"[INFO] Logging the parameters, textual info & best values for each metric & the model...\")\n",
        "mlflow_log(params, history, text_params, model,run_num)\n",
        "# Search for runs within the specified experiment\n",
        "mlflow.search_runs(experiment)\n",
        "# Define a final model based on the best parameters obtained from the experiment\n",
        "def final_model():\n",
        "  model=Sequential()\n",
        "  model.add(Conv2D(3,(3,3),padding='same',activation='relu',input_shape=(256,256,3)))\n",
        "  model.add(Conv2D(3,(3,3),padding='same',activation='relu'))\n",
        "  model.add(MaxPooling2D())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(110,activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.compile(optimizer=\"sgd\",loss=\"binary_crossentropy\",metrics=['acc'])\n",
        "  return model\n",
        "#Train and evaluate the model\n",
        "model=final_model()\n",
        "model.fit(train_iterator,validation_data=valid_iterator,epochs=8,batch_size=20,verbose=0)\n",
        "model.evaluate(test_iterator)\n",
        "# Save the model to a file using pickle.dump()\n",
        "with open('model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)"
      ],
      "metadata": {
        "id": "83-wVFYl-jUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}