{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGNb/A6c2suawxNe2RtYMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/IMDB_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c48UYWDcg3hR"
      },
      "source": [
        "# download IMDB dataset\n",
        "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"\n",
        "\n",
        "# list files in current directory\n",
        "!ls -lah"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "omGpTLIEzrY6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "AIwXjNLWIwYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBgArxzMy8_q"
      },
      "outputs": [],
      "source": [
        "# Read the data in pandas frame\n",
        "path_to_data=\"/content/movie_data.csv\"\n",
        "data=pd.read_csv(path_to_data)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the data\n",
        "print(\"This data contains: \"+str(data.shape[0])+\" rows and \"+str(data.shape[1])+\" columns.\")"
      ],
      "metadata": {
        "id": "Ib62rBMxy-K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of data over the classes\n",
        "data['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "TpW-pH4Hy-Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for nulls\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "8aRhF1Z-y-P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKKIsHqZwRJR"
      },
      "source": [
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# define cleaning function\n",
        "def clean_review(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove non alphabetic characters ^\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # stem words\n",
        "  # tokenize sentences\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Porter Stemmer\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  # reconstruct the text\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "  return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the cleaning_function on review column\n",
        "data['review']=data['review'].apply(clean_review)"
      ],
      "metadata": {
        "id": "FLI_pBPu0nc7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHlwVS71brN"
      },
      "source": [
        "# Split data into 70% training & 30% test\n",
        "X = data['review'].values\n",
        "y = data['sentiment'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_B0vrn-2sON"
      },
      "source": [
        "# define a CountVectorizer (with binary=True and max_features=10000)\n",
        "vectorizer = CountVectorizer(binary = True, max_features = 10000)\n",
        "\n",
        "# learn the vocabulary of all tokens in our training dataset\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "# transform x_train to bag of words\n",
        "x_train = vectorizer.transform(x_train)\n",
        "x_test = vectorizer.transform(x_test)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and fit your model(LogisticRegression)\n",
        "model=LogisticRegression()\n",
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "Md3fmUgby-Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "y_hat=model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "precision = precision_score(y_test, y_hat)\n",
        "recall = recall_score(y_test, y_hat)\n",
        "f1 = f1_score(y_test, y_hat)\n",
        "confusion = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "# Create a table report\n",
        "report = pd.DataFrame({\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
        "    \"Value\": [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "# Display the confusion matrix\n",
        "confusion_df = pd.DataFrame(confusion, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(report)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_df)\n"
      ],
      "metadata": {
        "id": "PXofrr4gLBQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6kxkZ5m55Ii"
      },
      "source": [
        "# define predict function\n",
        "def review_predictor(model, vectorizer, review):\n",
        "    review = clean_review(review)\n",
        "    review_bow = vectorizer.transform([review])\n",
        "    return model.predict(review_bow)[0]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try an example\n",
        "review=\"This movie was boring!\"\n",
        "review_predictor(model,vectorizer,review)"
      ],
      "metadata": {
        "id": "xf_X1juvy-Z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}