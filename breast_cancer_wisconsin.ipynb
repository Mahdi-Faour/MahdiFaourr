{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMySUauZrIrDrRH5D+4bOgL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/breast_cancer_wisconsin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5IH03aeyOo6"
      },
      "outputs": [],
      "source": [
        "#clone the repo.\n",
        "!git clone https://github.com/zaka-ai/machine_learning_certification\n",
        "#Import necessary libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "#Load in pandas dataframe.\n",
        "data=pd.read_csv(\"machine_learning_certification/breast-cancer-wisconsin.csv\")\n",
        "data.head()\n",
        "#Preprocesses and explores the data.\n",
        "data.info()\n",
        "print(\"The shape of our data is: \"+str(data.shape)+\" .\")\n",
        "encoder=LabelEncoder()\n",
        "data['Benign (2) or Malignant (4)']=encoder.fit_transform(data['Benign (2) or Malignant (4)'])\n",
        "data=data.drop('ID',axis=1)\n",
        "data['Benign (2) or Malignant (4)'].value_counts()\n",
        "data_Benign=data[(data['Benign (2) or Malignant (4)']==0)]\n",
        "data_Malignant=data[data['Benign (2) or Malignant (4)']==1]\n",
        "list_of_columns = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape']\n",
        "\n",
        "# Create subplots for each column.\n",
        "num_columns = len(list_of_columns)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_columns, figsize=(12,6))\n",
        "\n",
        "for i, column in enumerate(list_of_columns):\n",
        "    # Plot histograms for data_Benign and data_Malignant for each column.\n",
        "    ax = axes[i]\n",
        "    ax.hist(data_Benign[column], bins=20, alpha=0.5, label='Benign', color='blue')\n",
        "    ax.hist(data_Malignant[column], bins=20, alpha=0.5, label='Malignant', color='red')\n",
        "\n",
        "    # Customize the subplot\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title(f'Histogram of Benign and Malignant')\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the combined subplots\n",
        "plt.show()\n",
        "#Performs oversampling to address class imbalance.\n",
        "x=data.iloc[:,:-1].values\n",
        "y=data.iloc[:,-1].values\n",
        "over=RandomOverSampler(sampling_strategy=0.8)\n",
        "x,y=over.fit_resample(x,y)\n",
        "#Splits the data into training and testing sets.\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=42)\n",
        "print(\"Training data:\",x_train.shape,\",\",y_train.shape,\".\")\n",
        "print(\"---------------\")\n",
        "print(\"Testing data:\",x_test.shape,\",\",y_test.shape,\".\")\n",
        "#Trains and evaluates multiple machine learning models.\n",
        "models={\"LR\":LogisticRegression(),'RF':RandomForestClassifier(n_estimators=100),\n",
        "        \"ADAB\":AdaBoostClassifier(n_estimators=100),'SV':SVC(kernel='rbf')}\n",
        "for i in range(len(list(models))):\n",
        "  model=list(models.values())[i]\n",
        "  name=list(models.keys())[i]\n",
        "  model.fit(x_train,y_train)\n",
        "  y_pred=model.predict(x_test)\n",
        "  print(str(name)+\" scores for testing data:\")\n",
        "  print(\"accuracy % :\",accuracy_score(y_test,y_pred)*100,\".\")\n",
        "  print(\"precision % :\",precision_score(y_test,y_pred)*100,\".\")\n",
        "  print(\"recall % :\",recall_score(y_test,y_pred)*100,\".\")\n",
        "  print(\"-----------------\")\n",
        "  y_hat=model.predict(x_train)\n",
        "  print(str(name)+\" scores for training data:\")\n",
        "  print(\"accuracy % :\",accuracy_score(y_hat,y_train)*100,\".\")\n",
        "  print(\"precision % :\",precision_score(y_hat,y_train)*100,\".\")\n",
        "  print(\"recall % :\",recall_score(y_hat,y_train)*100,\".\")\n",
        "  print(\"----------------------------------------------------------\")\n",
        "#Performs cross-validation on AdaBoost and Random Forest models(the best two satisfying models ).\n",
        "  adaboost=AdaBoostClassifier(n_estimators=100)\n",
        "random_forest=RandomForestClassifier(n_estimators=100)\n",
        "adaboost_validation=cross_validate(adaboost,x,y,cv=4,scoring=['accuracy','precision','recall','f1'])\n",
        "random_forest_validation=cross_validate(random_forest,x,y,cv=4,scoring=['accuracy','precision','recall','f1'])\n",
        "print(\"Validation results for adaboost_clssifier:\")\n",
        "print(\"accuracy % :\",adaboost_validation.get(\"test_accuracy\").mean()*100,\".\")\n",
        "print(\"precision % :\",adaboost_validation.get(\"test_precision\").mean()*100,\".\")\n",
        "print(\"recall % :\",adaboost_validation.get(\"test_recall\").mean()*100,\".\")\n",
        "print(\"f1 % :\",adaboost_validation.get(\"test_f1\").mean()*100,\".\")\n",
        "print(\"-----------------------------\")\n",
        "print(\"Validation results for random_forest_clssifier:\")\n",
        "print(\"accuracy % :\",random_forest_validation.get(\"test_accuracy\").mean()*100,\".\")\n",
        "print(\"precision % :\",random_forest_validation.get(\"test_precision\").mean()*100,\".\")\n",
        "print(\"recall % :\",random_forest_validation.get(\"test_recall\").mean()*100,\".\")\n",
        "print(\"f1 % :\",random_forest_validation.get(\"test_f1\").mean()*100,\".\")\n",
        "# Create the AdaBoostClassifier again,to get into hyperparameter tuning process.\n",
        "adaboost = AdaBoostClassifier()\n",
        "\n",
        "# Define the parameter grid to search through.\n",
        "param_grid = {\n",
        "    'n_estimators': [50,75,100,125,150],  # Number of weak learners.\n",
        "    'learning_rate': [0.1, 0.5,0.7,0.4, 1.0]  # Learning rate.\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object.\n",
        "grid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to your data.\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score.\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score (Accuracy):\", best_score)\n",
        "\n",
        "# Use the best hyperparameters to create and evaluate the final model.\n",
        "best_model = AdaBoostClassifier(**best_params)\n",
        "best_model.fit(x_train, y_train)\n",
        "test_accuracy = best_model.score(x_test, y_test)\n",
        "print(\"Test Accuracy with Best Model:\", test_accuracy)\n",
        "\n",
        "# Save the model to a file.\n",
        "with open('best_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n"
      ]
    }
  ]
}