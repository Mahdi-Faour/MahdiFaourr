{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+6s8InOOfOVEDTr07IrQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/Obesity_Class_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interact with Hugging face API and curl the data"
      ],
      "metadata": {
        "id": "cjZmLeDs7KrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7KmfEMMIO8F"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \\\n",
        "     \"https://datasets-server.huggingface.co/rows?dataset=aiml2021%2Fobesity&config=default&split=train&offset=0&length=100\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \\\n",
        "     \"https://datasets-server.huggingface.co/splits?dataset=aiml2021%2Fobesity\""
      ],
      "metadata": {
        "id": "B3c8Xq9mIQi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \\\n",
        "     \"https://huggingface.co/api/datasets/aiml2021/obesity/parquet/default/train\""
      ],
      "metadata": {
        "id": "PAlG0GzeIQob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.metrics import Precision,F1Score\n",
        "from sklearn.model_selection import cross_val_score,train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "from sklearn.metrics import precision_score, accuracy_score,f1_score\n"
      ],
      "metadata": {
        "id": "b3Yh6qFL2pmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data in pandas\n",
        "data=pd.read_parquet(\"/content/0000.parquet\")"
      ],
      "metadata": {
        "id": "gwTuqu_rIQrZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a part from the data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "1aoZEA-_IQwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data-shape\n",
        "data.shape"
      ],
      "metadata": {
        "id": "jc5db4GCKTbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some info using info() method\n",
        "data.info()"
      ],
      "metadata": {
        "id": "G7vCegn3ZqPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the unique values with their count for the classes in NObeyesdad column\n",
        "data['NObeyesdad'].value_counts()"
      ],
      "metadata": {
        "id": "H-kVjcxJIQ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and display the data of categorical columns\n",
        "data_categorical=data[['Gender','family_history_with_overweight','FAVC','CAEC','SMOKE','SCC','CALC','MTRANS']]\n",
        "print(data_categorical.head())"
      ],
      "metadata": {
        "id": "kiKlMZjEIQ7Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and display the data of numerical columns\n",
        "data_numerical=data[['Age','Height','Weight','FCVC','NCP','CH2O','FAF','TUE']]\n",
        "print(data_numerical.head())"
      ],
      "metadata": {
        "id": "lApacf_VKNmu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'NObeyesdad' is the specific column we want to crosstab with\n",
        "target_column = 'NObeyesdad'\n",
        "\n",
        "# Get a list of all categorical columns excluding the target column\n",
        "columns=data_categorical.columns\n",
        "\n",
        "# Set up the figure and axes for subplots\n",
        "fig, axs = plt.subplots(len(columns), 1, figsize=(10, 5 * len(columns)))\n",
        "\n",
        "# Iterate over all categorical columns and create crosstab plots\n",
        "for i, column in enumerate(columns):\n",
        "    # Create a crosstab\n",
        "    crosstab_df = pd.crosstab(data[target_column], data_categorical[column])\n",
        "\n",
        "    # Plot the crosstab as a bar plot\n",
        "    crosstab_df.plot(kind='bar', ax=axs[i], stacked=True)\n",
        "\n",
        "    # Set plot titles and labels\n",
        "    axs[i].set_title(f'{column} vs {target_column}')\n",
        "    axs[i].set_xlabel(target_column)\n",
        "    axs[i].set_ylabel(column)\n",
        "\n",
        "# Adjust layout to prevent overlap of axis labels\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C2bsYF_1QO6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'NObeyesdad' is the specific column we want to analyze\n",
        "target_column = 'NObeyesdad'\n",
        "\n",
        "# Get a list of all numerical columns excluding the target column\n",
        "numerical_columns = data_numerical.columns\n",
        "\n",
        "# Iterate over all numerical columns and create individual plots\n",
        "for column in numerical_columns:\n",
        "    # Create a histogram for each category in 'NObeyesdad'\n",
        "    for category in data[target_column].unique():\n",
        "        fig, axs = plt.subplots(figsize=(8, 5))\n",
        "        axs.hist(data[data[target_column] == category][column], alpha=0.5, label=category)\n",
        "\n",
        "    # Set plot titles and labels\n",
        "        axs.set_title(f'{column} vs {target_column}')\n",
        "        axs.set_xlabel(column)\n",
        "        axs.set_ylabel('Frequency')\n",
        "        axs.legend()\n",
        "\n",
        "    # Show the plot for each numerical column\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "r0uGLNs_bzpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode all the categorical columns except the target column\n",
        "encoder=LabelEncoder()\n",
        "for column in data_categorical.columns:\n",
        "  data[column]=encoder.fit_transform(data_categorical[column])\n"
      ],
      "metadata": {
        "id": "jD1Zk-LszfhF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the target column\n",
        "data[target_column]=encoder.fit_transform(data[target_column])"
      ],
      "metadata": {
        "id": "h4KG6ttnDp4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the columns to normalize\n",
        "columns_to_normalize = [col for col in data.columns if col != target_column]\n",
        "\n",
        "# Calculate the maximum value excluding the specified column\n",
        "max_value_exclude_column = data[columns_to_normalize].max()\n",
        "\n",
        "# Normalize the selected columns\n",
        "data[columns_to_normalize] = data[columns_to_normalize].divide(max_value_exclude_column)\n"
      ],
      "metadata": {
        "id": "kUKo0GLWEQyK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the features and the labels in an arrays\n",
        "x=data.iloc[:,:-1].values\n",
        "y=data['NObeyesdad'].values"
      ],
      "metadata": {
        "id": "T-YzCMSW03CR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding of y\n",
        "y=to_categorical(y)"
      ],
      "metadata": {
        "id": "BdF-r9wP2nOk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing parts\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=15)\n",
        "\n",
        "# Define and compile your deep neural network model\n",
        "def create_model():\n",
        "   model=Sequential()\n",
        "   model.add(Dense(103,activation='relu',input_dim=x.shape[1]))\n",
        "   model.add(Dense(25,activation='relu'))\n",
        "   model.add(Dense(7,activation='softmax'))\n",
        "   model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['acc',Precision(),F1Score()])\n",
        "   return model"
      ],
      "metadata": {
        "id": "XeHnQkLZ1gXJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model=create_model()\n",
        "history=model.fit(x_train,y_train,batch_size=32,epochs=60,verbose=1)"
      ],
      "metadata": {
        "id": "pAyUZuYp_415"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on testing data\n",
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og4XnZRYAOZi",
        "outputId": "0636beb6-3d4c-424d-9a31-bec8eb283366"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 5ms/step - loss: 0.3137 - acc: 0.8962 - precision: 0.8962 - f1_score: 0.9040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31372544169425964,\n",
              " 0.8962264060974121,\n",
              " 0.8962264060974121,\n",
              " array([0.9166666 , 0.82758623, 0.9411765 , 0.984127  , 1.        ,\n",
              "        0.78378373, 0.87500006], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('model_obesity_NN.h5')"
      ],
      "metadata": {
        "id": "JvcsSdoB0UNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the features and the labels in an arrays again\n",
        "x=data.iloc[:,:-1].values\n",
        "y=data['NObeyesdad'].values"
      ],
      "metadata": {
        "id": "_01sO_-kKsh5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the classical models\n",
        "kneighbors = KNeighborsClassifier()\n",
        "LR = LogisticRegression()\n",
        "Adab = AdaBoostClassifier()\n",
        "svc = SVC(kernel='rbf')\n",
        "Forest = RandomForestClassifier()\n",
        "\n",
        "models = [ Adab,svc, Forest,LR,kneighbors]\n",
        "# Iterate over the models\n",
        "for model_name in models:\n",
        "    # Perform cross-validation\n",
        "    cv_results = cross_validate(model_name, x, y, scoring=['precision_macro', 'accuracy', 'f1_macro'], cv=5)\n",
        "\n",
        "    # Extract and print the evaluation metrics\n",
        "    precision = np.mean(cv_results['test_precision_macro'])\n",
        "    accuracy = np.mean(cv_results['test_accuracy'])\n",
        "    f1 = np.mean(cv_results['test_f1_macro'])\n",
        "    #recall=np.mean(cv_results['test_recall'])\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    #print(f\"Recall Score: {recall}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FPCiLPKHWuC",
        "outputId": "e609d80b-7473-4dc6-f818-20af27b80988"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: AdaBoostClassifier()\n",
            "Precision: 0.2569205958853007\n",
            "Accuracy: 0.29985210581156935\n",
            "F1 Score: 0.20146565036523256\n",
            "-----\n",
            "Model: SVC()\n",
            "Precision: 0.7616778728434797\n",
            "Accuracy: 0.745238815502\n",
            "F1 Score: 0.7373964839506232\n",
            "-----\n",
            "Model: RandomForestClassifier()\n",
            "Precision: 0.9563919674821886\n",
            "Accuracy: 0.9356727504957817\n",
            "F1 Score: 0.9358335860302149\n",
            "-----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: LogisticRegression()\n",
            "Precision: 0.6601613121041032\n",
            "Accuracy: 0.6632572574591331\n",
            "F1 Score: 0.6418390354718732\n",
            "-----\n",
            "Model: KNeighborsClassifier()\n",
            "Precision: 0.7538691645913295\n",
            "Accuracy: 0.7547085252036346\n",
            "F1 Score: 0.7397142844565833\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    # Add more hyperparameters as needed\n",
        "}\n",
        "\n",
        "# Create GridSearchCV for the model with best performance\n",
        "grid_search = GridSearchCV(Forest, param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x,y)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "FspR-Lsxzfug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a8f4b62-f777-48aa-d227-e336e08e7d43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best Model Accuracy: 0.7547085252036346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tuned model\n",
        "Tuned_Forest_Model=RandomForestClassifier(max_depth=20,min_samples_split=2,n_estimators=100,min_samples_leaf=1)"
      ],
      "metadata": {
        "id": "V3nFdQspbzro"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the tuned model\n",
        "Tuned_Forest_Model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "ZcoTkncMVJ1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file using joblib\n",
        "joblib.dump(Tuned_Forest_Model, 'obesity_model_Forest.joblib')"
      ],
      "metadata": {
        "id": "kOBxzkP_1bIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on testing data\n",
        "y_pred=Tuned_Forest_Model.predict(x_test)\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "precision=precision_score(y_test,y_pred,average='macro')\n",
        "f1score=f1_score(y_test,y_pred,average='macro')\n",
        "\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "print(\"Precision: {:.2%}\".format(precision))\n",
        "print(\"F1 Score: {:.2%}\".format(f1score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p835EiScVJ3g",
        "outputId": "eb29b24a-1a02-4539-f262-421390b47d6f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.09%\n",
            "Precision: 98.47%\n",
            "F1 Score: 94.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class predictor function and blend the predictions of the deep NN model and the Forest model\n",
        "def weight_class_function(input, model, Tuned_Forest_Model):\n",
        "\n",
        "    input = np.expand_dims(input, axis=0)  # Add a batch dimension\n",
        "\n",
        "    # Make predictions using the deep learning model\n",
        "    pred1 = model.predict(input)\n",
        "\n",
        "    # Make predictions using the RandomForest model\n",
        "    pred2 = Tuned_Forest_Model.predict(input)\n",
        "\n",
        "    # Blend the predictions (simple averaging in this case)\n",
        "    blended_pred = (np.argmax(pred1, axis=1) + pred2) / 2\n",
        "\n",
        "    return np.argmax(blended_pred,axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gn7x6r1KYqu2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the classes of some instances\n",
        "print(data.iloc[100,-1])\n",
        "print(data.iloc[1050,-1])\n",
        "print(data.iloc[243,-1])\n",
        "print(data.iloc[2001,-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quBeKrEfc8P7",
        "outputId": "241e855c-544b-49a9-ab33-8997da33e00e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "6\n",
            "6\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the  weight_class_function on the choosen random samples\n",
        "evaluating_examples=[data.iloc[100,:-1].values,data.iloc[1050,:-1].values,data.iloc[243,:-1].values,data.iloc[2001,:-1].values]\n",
        "for example in evaluating_examples:\n",
        " input=example\n",
        " print(weight_class_function(input,model,Tuned_Forest_Model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msh7Zcy3eZPY",
        "outputId": "cd71b79f-be21-4054-ba78-93eaa16f5e4f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "[1]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[6]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[6]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[4]\n"
          ]
        }
      ]
    }
  ]
}