{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl4EQZyNsA1n6u27pNi9yR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/makeup_nomakeup_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "hply-XzI9Sby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import opendatasets as od\n",
        "import os\n",
        "import shutil\n",
        "from keras.metrics import Precision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ],
      "metadata": {
        "id": "mj09xn5N9Sf5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/petersunga/make-up-vs-no-make-up\")"
      ],
      "metadata": {
        "id": "rhtFjc8r9Skt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TZrscnel9Qhj"
      },
      "outputs": [],
      "source": [
        "# Set some directories\n",
        "classes = ['makeup', 'no_makeup']\n",
        "train_directory = \"/content/train\"\n",
        "test_directory = \"/content/test\"\n",
        "root_directory = \"/content/make-up-vs-no-make-up/data\"\n",
        "\n",
        "# Iterate over classes\n",
        "for class_name in classes:\n",
        "    class_directory = os.path.join(root_directory, class_name)\n",
        "    images = os.listdir(class_directory) # List all images in the targeted path\n",
        "\n",
        "    # Split the images into training and testing sets\n",
        "    train_images, test_images = train_test_split(images, test_size=0.1)\n",
        "\n",
        "    # Create destination directories\n",
        "    train_class_dir = os.path.join(train_directory, class_name)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "\n",
        "    test_class_dir = os.path.join(test_directory, class_name)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Copy training images\n",
        "    for image in train_images:\n",
        "        source_path = os.path.join(class_directory, image)\n",
        "        destination_path = os.path.join(train_class_dir, image)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "    # Copy testing images\n",
        "    for image in test_images:\n",
        "        source_path = os.path.join(class_directory, image)\n",
        "        destination_path = os.path.join(test_class_dir, image)\n",
        "        shutil.copy(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ImageDataGenerator for data augmentation\n",
        "datagen=ImageDataGenerator(rescale=1.0/255.0,\n",
        "      rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2)\n",
        "\n",
        "train_generator=datagen.flow_from_directory(\"/content/train\",class_mode=\"binary\",batch_size=32,target_size=(224,224))\n",
        "test_generator=datagen.flow_from_directory(\"/content/test\",class_mode=\"binary\",batch_size=32,target_size=(224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX_MzKhe9SqG",
        "outputId": "540f1ce3-545b-40d2-ccef-2a9e6b7aa4be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1354 images belonging to 2 classes.\n",
            "Found 152 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_metric=Precision()"
      ],
      "metadata": {
        "id": "-WoHnnCXswdf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MobileNetV1 base model without top (fully connected) layers\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the MobileNetV1 base model to the Sequential model\n",
        "model.add(base_model)\n",
        "# Set MobileNetV1 layers as non-trainable\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add GlobalAveragePooling2D layer to reduce spatial dimensions\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Add custom dense layers for classification\n",
        "model.add(Dense(220, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy',precision_metric])"
      ],
      "metadata": {
        "id": "mSeAlOy4B2ib"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a callback to save the weights when precision is highest\n",
        "checkpoint = ModelCheckpoint('best_weights.h5',\n",
        "                             monitor='val_precision_3',\n",
        "                             save_best_only=True,\n",
        "                             mode='max',\n",
        "                             verbose=1)"
      ],
      "metadata": {
        "id": "A2rLSr_sB2ku"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_generator,validation_data=(test_generator),epochs=20,batch_size=len(train_generator),callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "pkiglxoLB2gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57438b43-3797-447d-e965-e6b3bff58a11"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.6972 - precision_3: 0.5036\n",
            "Epoch 1: val_precision_3 improved from -inf to 0.80000, saving model to best_weights.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r43/43 [==============================] - 90s 2s/step - loss: 0.5934 - accuracy: 0.6972 - precision_3: 0.5036 - val_loss: 0.5502 - val_accuracy: 0.7237 - val_precision_3: 0.8000\n",
            "Epoch 2/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.7422 - precision_3: 0.5969\n",
            "Epoch 2: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 86s 2s/step - loss: 0.4979 - accuracy: 0.7422 - precision_3: 0.5969 - val_loss: 0.4848 - val_accuracy: 0.7895 - val_precision_3: 0.7097\n",
            "Epoch 3/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4403 - accuracy: 0.7747 - precision_3: 0.6469\n",
            "Epoch 3: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 89s 2s/step - loss: 0.4403 - accuracy: 0.7747 - precision_3: 0.6469 - val_loss: 0.6319 - val_accuracy: 0.7171 - val_precision_3: 0.7500\n",
            "Epoch 4/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.7792 - precision_3: 0.6592\n",
            "Epoch 4: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 85s 2s/step - loss: 0.4447 - accuracy: 0.7792 - precision_3: 0.6592 - val_loss: 0.5369 - val_accuracy: 0.7105 - val_precision_3: 0.5455\n",
            "Epoch 5/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.7976 - precision_3: 0.6900\n",
            "Epoch 5: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 82s 2s/step - loss: 0.4276 - accuracy: 0.7976 - precision_3: 0.6900 - val_loss: 0.4867 - val_accuracy: 0.7763 - val_precision_3: 0.6341\n",
            "Epoch 6/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3992 - accuracy: 0.8035 - precision_3: 0.7111\n",
            "Epoch 6: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3992 - accuracy: 0.8035 - precision_3: 0.7111 - val_loss: 0.4665 - val_accuracy: 0.7763 - val_precision_3: 0.6774\n",
            "Epoch 7/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8109 - precision_3: 0.7072\n",
            "Epoch 7: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 85s 2s/step - loss: 0.4030 - accuracy: 0.8109 - precision_3: 0.7072 - val_loss: 0.5168 - val_accuracy: 0.7434 - val_precision_3: 0.6500\n",
            "Epoch 8/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8146 - precision_3: 0.7164\n",
            "Epoch 8: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 80s 2s/step - loss: 0.3808 - accuracy: 0.8146 - precision_3: 0.7164 - val_loss: 0.4635 - val_accuracy: 0.7961 - val_precision_3: 0.6842\n",
            "Epoch 9/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8117 - precision_3: 0.7057\n",
            "Epoch 9: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3910 - accuracy: 0.8117 - precision_3: 0.7057 - val_loss: 0.4535 - val_accuracy: 0.7500 - val_precision_3: 0.6000\n",
            "Epoch 10/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8323 - precision_3: 0.7416\n",
            "Epoch 10: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3594 - accuracy: 0.8323 - precision_3: 0.7416 - val_loss: 0.4608 - val_accuracy: 0.7961 - val_precision_3: 0.7188\n",
            "Epoch 11/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.7962 - precision_3: 0.6752\n",
            "Epoch 11: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.4153 - accuracy: 0.7962 - precision_3: 0.6752 - val_loss: 0.4207 - val_accuracy: 0.8158 - val_precision_3: 0.7429\n",
            "Epoch 12/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3498 - accuracy: 0.8397 - precision_3: 0.7514\n",
            "Epoch 12: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3498 - accuracy: 0.8397 - precision_3: 0.7514 - val_loss: 0.4534 - val_accuracy: 0.7895 - val_precision_3: 0.6857\n",
            "Epoch 13/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8412 - precision_3: 0.7584\n",
            "Epoch 13: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3413 - accuracy: 0.8412 - precision_3: 0.7584 - val_loss: 0.6015 - val_accuracy: 0.7697 - val_precision_3: 0.7500\n",
            "Epoch 14/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8530 - precision_3: 0.7717\n",
            "Epoch 14: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.3224 - accuracy: 0.8530 - precision_3: 0.7717 - val_loss: 0.5089 - val_accuracy: 0.7500 - val_precision_3: 0.5814\n",
            "Epoch 15/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8523 - precision_3: 0.7668\n",
            "Epoch 15: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 85s 2s/step - loss: 0.3200 - accuracy: 0.8523 - precision_3: 0.7668 - val_loss: 0.4346 - val_accuracy: 0.7632 - val_precision_3: 0.5918\n",
            "Epoch 16/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8560 - precision_3: 0.7914\n",
            "Epoch 16: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 85s 2s/step - loss: 0.3082 - accuracy: 0.8560 - precision_3: 0.7914 - val_loss: 0.5154 - val_accuracy: 0.7895 - val_precision_3: 0.7241\n",
            "Epoch 17/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8744 - precision_3: 0.8154\n",
            "Epoch 17: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 83s 2s/step - loss: 0.2879 - accuracy: 0.8744 - precision_3: 0.8154 - val_loss: 0.4710 - val_accuracy: 0.8026 - val_precision_3: 0.7419\n",
            "Epoch 18/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8671 - precision_3: 0.8017\n",
            "Epoch 18: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 86s 2s/step - loss: 0.3082 - accuracy: 0.8671 - precision_3: 0.8017 - val_loss: 0.5006 - val_accuracy: 0.7697 - val_precision_3: 0.5962\n",
            "Epoch 19/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8575 - precision_3: 0.7799\n",
            "Epoch 19: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 86s 2s/step - loss: 0.3066 - accuracy: 0.8575 - precision_3: 0.7799 - val_loss: 0.4929 - val_accuracy: 0.8026 - val_precision_3: 0.6667\n",
            "Epoch 20/20\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.8722 - precision_3: 0.8156\n",
            "Epoch 20: val_precision_3 did not improve from 0.80000\n",
            "43/43 [==============================] - 80s 2s/step - loss: 0.2855 - accuracy: 0.8722 - precision_3: 0.8156 - val_loss: 0.5583 - val_accuracy: 0.7697 - val_precision_3: 0.6136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model with best weights\n",
        "mdl=load_model(\"/content/best_weights.h5\")"
      ],
      "metadata": {
        "id": "-OHF_vuICjZT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test_generator\n",
        "mdl.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "398P2wtlB2m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401edda1-d84d-427f-bb86-88b4a292c509"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 10s 1s/step - loss: 0.5655 - accuracy: 0.7105 - precision_3: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5655026435852051, 0.7105262875556946, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makeup_detector(img_path):\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Adjust target_size as needed\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, 0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Normalize pixel values to the range [0, 1]\n",
        "    # Make predictions\n",
        "    predictions = mdl.predict(img_array)\n",
        "\n",
        "    # The model has two classes (makeup and no_makeup)\n",
        "    class_labels = ['no_makeup', 'makeup']\n",
        "    if predictions[0,0]>0.5:\n",
        "      print(f\"The predicted class for the given image is: {class_labels[1]}\")\n",
        "    else:\n",
        "      print(f\"The predicted class for the given image is: {class_labels[0]}\")"
      ],
      "metadata": {
        "id": "s3g-2NoGJuuJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "img_path=\"/content/test/no_makeup/no_makeup999.jpeg\"\n",
        "makeup_detector(img_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vyCVMHGJuzh",
        "outputId": "8e7dfabd-d78d-4f3f-e40e-d527a3e3e362"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n",
            "The predicted class for the given image is: no_makeup\n"
          ]
        }
      ]
    }
  ]
}