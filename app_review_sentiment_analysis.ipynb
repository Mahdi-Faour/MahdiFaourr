{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeWN9M98FHcNFgtHDXyFaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/app_review_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Curl the Data and Upload it in the Working Directory**\n"
      ],
      "metadata": {
        "id": "P-lzNxA-On-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23RngnYj5b8t"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \\\n",
        "     \"https://datasets-server.huggingface.co/rows?dataset=app_reviews&config=default&split=train&offset=0&length=100\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \\\n",
        "     \"https://datasets-server.huggingface.co/splits?dataset=app_reviews\""
      ],
      "metadata": {
        "id": "8Z8WEwZG5dMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \\\n",
        "     \"https://huggingface.co/api/datasets/app_reviews/parquet/default/train\""
      ],
      "metadata": {
        "id": "F8k3H64F5dO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.metrics import Precision\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6NWSkUMxMxmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define necessary objects\n",
        "precision_metric = Precision()\n",
        "stemmer=PorterStemmer()\n",
        "English_stopwords=stopwords.words('english')"
      ],
      "metadata": {
        "id": "bgziGON5QIeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "oeK3XskGRI0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the parquet file in a pandas dataframe\n",
        "df = pd.read_parquet('/content/0000.parquet')\n",
        "# Dsiplay a part from the data\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "qsgAqdAg5dUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "WKuxG_7I8WjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check nulls\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ab48dm6J8A3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number and the distribution of labels\n",
        "df['star'].value_counts()"
      ],
      "metadata": {
        "id": "AXBXD9s7t-ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "zvBMJabhRs5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features =df['review'].values\n",
        "stars =df['star'].values\n",
        "\n",
        "# Specify the desired number of instances for each class\n",
        "desired_instances_per_class = {\n",
        "    1: 50000,  # Desired number for class 1\n",
        "    2: 50000,  # Desired number for class 2\n",
        "    3: 50000,  # Desired number for class 3\n",
        "    4: 50000,  # Desired number for class 4\n",
        "    5:1743  # Desired number for class 5\n",
        "}\n",
        "\n",
        "# Initialize lists to store oversampled data\n",
        "oversampled_features = []\n",
        "oversampled_stars = []\n",
        "\n",
        "# Loop through each class\n",
        "for current_class in range(1, 6):  #  the ratings are from 1 to 5\n",
        "    # Get indices of instances belonging to the current class\n",
        "    indices = np.where(stars == current_class)[0]\n",
        "\n",
        "    # Calculate the number of instances to duplicate to achieve the desired count\n",
        "    num_to_duplicate = desired_instances_per_class[current_class] - len(indices)\n",
        "\n",
        "    # Randomly duplicate instances\n",
        "    if num_to_duplicate > 0:\n",
        "        duplicated_indices = np.random.choice(indices, num_to_duplicate, replace=True)\n",
        "        oversampled_features.extend(features[duplicated_indices])\n",
        "        oversampled_stars.extend(stars[duplicated_indices])\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "oversampled_features = np.array(oversampled_features)\n",
        "oversampled_stars = np.array(oversampled_stars)\n",
        "\n",
        "# Create a DataFrame\n",
        "df_new_samples= pd.DataFrame({'review': oversampled_features, 'star': oversampled_stars})\n",
        "# Combine the original data with the created one\n",
        "data = pd.concat([df_new_samples, df[['review','star']]], ignore_index=True)"
      ],
      "metadata": {
        "id": "jCjfrG8yK5wn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that process texts\n",
        "def text_cleaner(text):\n",
        "  text=text.lower()# Convert to lower cases\n",
        "  text_with_no_punctuations = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Remove non alphabatic symbols\n",
        "  tokens=word_tokenize(text_with_no_punctuations) # tokeize words\n",
        "  stemmed_text = [stemmer.stem(word) for word in tokens] # Apply stemming\n",
        "  text = ' '.join(stemmed_text)\n",
        "  text_with_no_stopwords=[word for word in text.split() if word not in English_stopwords]# remove english stopwords\n",
        "  final_cleaned_text=' '.join(text_with_no_stopwords)\n",
        "  return final_cleaned_text\n"
      ],
      "metadata": {
        "id": "AFQJ-zpL9LAa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that returns the length of a text\n",
        "def count_words(text):\n",
        "  return len(text.split())"
      ],
      "metadata": {
        "id": "ks7iYo3t9LDu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text_cleaner on review column\n",
        "data['cleaned_review']=data['review'].apply(text_cleaner)"
      ],
      "metadata": {
        "id": "wqWSx6hW9LjC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply count_words function on the cleaned_review column\n",
        "data['sentence_length']=data['cleaned_review'].apply(count_words)"
      ],
      "metadata": {
        "id": "J4obPLp78BAZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some statistics for the senetence_length column\n",
        "data['sentence_length'].describe()"
      ],
      "metadata": {
        "id": "cfu7FArI5dae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer object\n",
        "tokenizer_object=Tokenizer(num_words=10000)\n",
        "tokenizer_object.fit_on_texts(data['cleaned_review'])# fit on cleaned_review column"
      ],
      "metadata": {
        "id": "j4qdJ-Xs-AJj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the vocab_size\n",
        "tokenizer_object.word_index\n",
        "vocab_size=len(tokenizer_object.word_index)+1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "MWM9SNPN-ANG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features and the labels (in array formats)\n",
        "x=tokenizer_object.texts_to_sequences(data['cleaned_review'])\n",
        "y=data['star'].values\n",
        "padded_x= pad_sequences(x, maxlen=50, padding='post', truncating='post')# pad the arrays up to the same length"
      ],
      "metadata": {
        "id": "WgjBBXqs5dd8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=LabelEncoder()# Define encoder_object\n",
        "encoded_y=encoder.fit_transform(y)# Encode the labels\n",
        "y=to_categorical(encoded_y) # Perform one hot encoding"
      ],
      "metadata": {
        "id": "rfH8I-XG5dgu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing parts\n",
        "x_train,x_test,y_train,y_test=train_test_split(padded_x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "zsWPbYBEK50I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling**"
      ],
      "metadata": {
        "id": "VxUSHuj7Udmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model and use LSTM layer\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=50))\n",
        "model.add(LSTM(125,return_sequences=False))\n",
        "model.add(Dense(200,activation='relu'))\n",
        "model.add(Dense(45,activation='relu'))\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"acc\",precision_metric])\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=64,epochs=20)"
      ],
      "metadata": {
        "id": "8Y99glVOK5jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit the data into the model\n",
        "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"acc\",precision_metric])\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=64,epochs=20)"
      ],
      "metadata": {
        "id": "xXmwjTEcUnyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for demo\n",
        "def rate_function(text):\n",
        "  # clean the input text\n",
        "    cleaned_text = text_cleaner(text)\n",
        "  # convert the cleaned text to a sequence of integers\n",
        "    text_array = tokenizer_object.texts_to_sequences([cleaned_text])\n",
        "  # pad the sequence\n",
        "    padded_array = pad_sequences(text_array, maxlen=50, padding='post', truncating='post')\n",
        "  # use the model created to generate predictions\n",
        "    prediction = model.predict(padded_array)\n",
        "\n",
        "    # Find the predicted class\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    return predicted_class\n"
      ],
      "metadata": {
        "id": "Uyfhgg9sgJ-l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"very restricted app with poor resources.\"\n",
        "rate_function(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9xyTFHyB10t",
        "outputId": "10b6415c-45d0-4add-c771-099fded22154"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your model\n",
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "AlfEZOKwxhCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the tokenizer to a file using pickle\n",
        "with open('tokenizer_object.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer_object, f)"
      ],
      "metadata": {
        "id": "UH4V_P_OgKIc"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}